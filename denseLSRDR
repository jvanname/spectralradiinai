# Here, aa will be a list of matrices. bb will be trained by gradient ascent to converge to an L_{2,d}-spectral radius dimensionality reduction (LSRDR) of aa.
# To run the code just copy and paste into Julia.

# If the type is real, since we are only approximating the dominant eigenvectors using power iteration, if the two largest eigenvalues of the spectral radius of sum(kron.(aa,adjoint.(transpose.(bb)))) are complex conjugates with one another, then the power iteration will not accurately approximate either of these dominant eigenvectors. This is a good thing. Since the gradient ascent process uses these dominant eigenvectors and the dominant eigenvectors are not accurate, the gradient ascent will only maximize the fitness in the case here sum(kron.(aa,adjoint.(transpose.(bb)))) has a single dominant eigenvalue. But when sum(kron.(aa,adjoint.(transpose.(bb)))) has a single dominant eigenvalue, the fitness of bb will be higher than if sum(kron.(aa,adjoint.(transpose.(bb)))) has two conjugate dominant eigenvalues. Therefore, our version of gradient ascent can avoid local maxima which are not global maxima in the case when sum(kron.(aa,adjoint.(transpose.(bb)))) has two conjugate dominant eigenvalues.

using Statistics,LinearAlgebra;

n=50;
d=20;
k=10;

type=ComplexF64;

aa=Array{Array{type,2},1}(undef,k);
bb=Array{Array{type,2},1}(undef,k);
rr=randn(type,d,n);;
ss=randn(type,n,d);;
for i in 1:k
aa[i]=randn(type,n,n);
bb[i]=rr*aa[i]*ss;
end;

#######################################################################################################################################################
# Here we compute LSRDRs without gradient descent.

# The function f is a low degree polynomial function with an attractive fixed point at 0 and 1. Iteratively applying this function to a matrix produces a projection matrix.
function f(x) return 3*x^2-2*x^3; end;


function quantop(x) 
return sum(map(v->v*x*adjoint(v),aa)); end;

function adjointquantop(x) 
return sum(map(v->adjoint(v)*x*v,aa)); end;

##############################################
# Here, proj has low rank, but we do not factorize proj.

proj=randn(type,n,n)*0;
for i in 1:d proj[i,i]=1; end;
va=randn(type,n,n);
va-=adjoint(va);
va=exp(va);
proj=va*proj*adjoint(va);

G=randn(type,n,n); H=randn(type,n,n); 
rate=0.0001;
vf=proj;
oldvf=proj;
while true
GG=adjointquantop(G*proj)*proj;
G=GG/tr(GG);
HH=quantop(H*adjoint(proj))*adjoint(proj);
H=HH/tr(HH);
oldproj=proj
proj=f(f(f(f(f(f(f(f(proj+rate*(proj*adjoint(G)+H*proj)))))))));
qh=norm(proj-oldproj);
oldvf=vf;
vf=proj-oldproj;
if abs(dot(oldvf,vf))/(norm(oldvf)*norm(vf))<0.99998 rate*=0.99; else rate*=1.01; end;
display([rate,qh]);
if qh<10^(-14) break; end;
end;

####################################################
# Here, we keep proj factorized as a product ss*rr of low rank matrices during training.
# Note. I have not used any adaptive learning rate here, so one may need to adjust the learning rate.

rr=Matrix(I,d,n);
ss=Matrix(I,n,d);
G=randn(type,n,n); H=randn(type,n,n); 
rate=0.0001;
cc=0;
rrgrad=rr;
ssgrad=ss;
panic=[];
while true
GG=sum(map(v->adjoint(v)*G*ss*rr*v*ss*rr,aa));
G=GG/tr(GG);
HH=sum(map(v->v*H*adjoint(ss*rr*v*ss*rr),aa));
H=HH/tr(HH);
rate=0.2;
oldss=deepcopy(ss);
oldrr=deepcopy(rr);
ss+=rate*H*ss;
rr+=rate*rr*adjoint(G);
for jj in 1:8
rr=3*rr*ss*rr-2*rr*ss*rr*ss*rr;
ss=3*ss*rr*ss-2*ss*rr*ss*rr*ss;
ss,rr=ss*rr*ss,3*rr-2*rr*ss*rr;
ss,rr=3*ss-2*ss*rr*ss,rr*ss*rr;
end;
vv=adjoint(ss)*ss-rr*adjoint(rr);
ss=ss*(Matrix(I,d,d)+rate*vv/10)^(-1)
rr=(Matrix(I,d,d)+rate*vv/10)*rr;
display(abs(dot(oldrr*adjoint(rr),adjoint(ss)*ss)));
end;




#######################################################################################################################################################
#######################################################################################################################################################
# Below we use gradient descent to produce LSRDRs.

# These will approximate the dominant eigenvectors needed to compute the gradients of the spectral radii.

toplefteigvec=randn(type,n,d);
toprighteigvec=randn(type,n,d);
bottomlefteigvec=randn(type,d,d);
bottomrighteigvec=randn(type,d,d);

rate=.8;
grad=bb*0;
pp=0;
qq=0;
while true
 newtoplefteigvec=zeros(type,n,d);
 newtoprighteigvec=zeros(type,n,d);
 newbottomlefteigvec=zeros(type,d,d);
 newbottomrighteigvec=zeros(type,d,d);
# We use power iteration to update the left and right eigenvectors and eigenvalues.
 for i in 1:k
 newtoplefteigvec+=adjoint(aa[i])*toplefteigvec*bb[i];
 newtoprighteigvec+=aa[i]*toprighteigvec*adjoint(bb[i]);
 newbottomlefteigvec+=adjoint(bb[i])*bottomlefteigvec*bb[i];
 newbottomrighteigvec+=bb[i]*bottomrighteigvec*adjoint(bb[i]);
 end;
 toplambda=dot(toprighteigvec,newtoprighteigvec);
 bottomlambda=dot(bottomrighteigvec,newbottomrighteigvec);
 #The following two lines of code are needed if we want the average of the toplefteigvec and 
 # toprighteigvec to be zero. This is useful for cryptography applications.
 #newtoplefteigvec-=ones(n)*(adjoint(ones(n))*(newtoplefteigvec/n));
 #newtoprighteigvec-=ones(n)*(adjoint(ones(n))*(newtoprighteigvec/n));

 toplefteigvec=newtoplefteigvec/norm(newtoplefteigvec);
 toprighteigvec=newtoprighteigvec/norm(newtoprighteigvec);
 bottomlefteigvec=newbottomlefteigvec/norm(newbottomlefteigvec);
 bottomrighteigvec=newbottomrighteigvec/norm(newbottomrighteigvec);
 grad*=0.95;
 oldgrad=deepcopy(grad);
for i in 1:k
# This is the gradient.
grad[i]+=0.05*adjoint(toplefteigvec)*aa[i]*toprighteigvec*adjoint(toplambda)*dot(toprighteigvec,toplefteigvec)/abs(toplambda*dot(toprighteigvec,toplefteigvec))^2;
 grad[i]-=0.05*bottomlefteigvec*bb[i]*adjoint(bottomrighteigvec)*dot(bottomlefteigvec,bottomrighteigvec)*bottomlambda/abs(dot(bottomlefteigvec,bottomrighteigvec)*bottomlambda)^2;
#The following line is commented since it is equal to the above line.
#grad[i]- =0.05*adjoint(bottomlefteigvec)*bb[i]*bottomrighteigvec*adjoint(bottomlambda)*dot(bottomrighteigvec,bottomlefteigvec)/abs(bottomlambda*dot(bottomrighteigvec,bottomlefteigvec))^2;
 end;

 bb+=rate*grad;

 bb/=mean(norm.(bb));
 qq=pp;
 pp=log(abs(toplambda/abs(bottomlambda)^(1/2)));
 display([pp,rate]);
 if pp==qq break; end;
if abs(dot(grad,oldgrad))/(norm(grad)*norm(oldgrad))<0.999 rate*=0.99; else rate*=1.01; end;
end;

##################################################################################################
##################################################################################################
##################################################################################################
##################################################################################################

# Here is the code for finding an LSRDR projector rr,ss which is trained by gradient ascent.

toplefteigvec=randn(type,n,d);
toprighteigvec=randn(type,n,d);
bottomlefteigvec=randn(type,d,d);
bottomrighteigvec=randn(type,d,d);
rr=randn(type,d,n);
ss=randn(type,n,d);

rrgrad=rr*0;
ssgrad=ss*0;

rate=.5;
pp=0;
qq=-20;
while true
newtoplefteigvec=zeros(type,n,d);
newtoprighteigvec=zeros(type,n,d);

newbottomlefteigvec=zeros(type,d,d);
newbottomrighteigvec=zeros(type,d,d);

for i in 1:k
newtoplefteigvec+=adjoint(aa[i])*toplefteigvec*bb[i];
newtoprighteigvec+=aa[i]*toprighteigvec*adjoint(bb[i]);
newbottomlefteigvec+=adjoint(bb[i])*bottomlefteigvec*bb[i];
newbottomrighteigvec+=bb[i]*bottomrighteigvec*adjoint(bb[i])
end;
toplambda=dot(toprighteigvec,newtoprighteigvec);
bottomlambda=dot(bottomrighteigvec,newbottomrighteigvec);
# newtoplefteigvec-=ones(n)*(adjoint(ones(n))*(newtoplefteigvec/n));
# newtoprighteigvec-=ones(n)*(adjoint(ones(n))*(newtoprighteigvec/n));
#for i in 1:d
#rr[i,:]-=ones(n)*mean(rr[i,:]);
#ss[:,i]-=ones(n)*mean(ss[:,i]);
#end;

toplefteigvec=newtoplefteigvec/norm(newtoplefteigvec);
toprighteigvec=newtoprighteigvec/norm(newtoprighteigvec);
bottomlefteigvec=newbottomlefteigvec/norm(newbottomlefteigvec);
bottomrighteigvec=newbottomrighteigvec/norm(newbottomrighteigvec);

# I have verified that the eigenvectors and eigenvalues are good.

#slowgrad=gradient(sr,aa,bb)[2]-sum(gradient(sr,bb,bb))/2;

rrgrad*=0.95;
ssgrad*=0.95;
oldrrgrad=deepcopy(rrgrad);
oldssgrad=deepcopy(ssgrad);
for i in 1:k
rrgrad+=0.05*adjoint(toplefteigvec)*aa[i]*toprighteigvec*adjoint(toplambda)*dot(toprighteigvec,toplefteigvec)/abs(toplambda*dot(toprighteigvec,toplefteigvec))^2*adjoint(aa[i]*ss);
ssgrad+=0.05*adjoint(rr*aa[i])*adjoint(toplefteigvec)*aa[i]*toprighteigvec*adjoint(toplambda)*dot(toprighteigvec,toplefteigvec)/abs(toplambda*dot(toprighteigvec,toplefteigvec))^2;

rrgrad-=0.05*bottomlefteigvec*bb[i]*adjoint(bottomrighteigvec)*dot(bottomlefteigvec,bottomrighteigvec)*bottomlambda/abs(dot(bottomlefteigvec,bottomrighteigvec)*bottomlambda)^2*adjoint(aa[i]*ss);
ssgrad-=0.05*adjoint(rr*aa[i])*bottomlefteigvec*bb[i]*adjoint(bottomrighteigvec)*dot(bottomlefteigvec,bottomrighteigvec)*bottomlambda/abs(dot(bottomlefteigvec,bottomrighteigvec)*bottomlambda)^2;
end;

#display([gradient(super,aa,rr,ss)[2]-rrgrad,gradient(super,aa,rr,ss)[3]-ssgrad])

rr+=rate*rrgrad;
ss+=rate*ssgrad;

rr/=norm(rr);
ss/=norm(ss);

for i in 1:k
bb[i]=rr*aa[i]*ss;
end;

qq=pp;
pp=log(abs(toplambda/abs(bottomlambda)^(1/2)));
display([pp,rate]);

if abs(dot(rrgrad,oldrrgrad))/(norm(rrgrad)*norm(oldrrgrad))<0.9999 rate*=0.995; else rate*=1.005; end;
if abs(dot(ssgrad,oldssgrad))/(norm(ssgrad)*norm(oldssgrad))<0.9999 rate*=0.995; else rate*=1.005; end;

end;

##################################################################################################
##################################################################################################
##################################################################################################
##################################################################################################

#qh=bottomrighteigvec/tr(bottomrighteigvec);
#qh+=adjoint(qh);
#qh=qh^(1/2);
#ph=qh^(-1);
#for i in 1:k
#bb[i]=ph*bb[i]*qh;
#end;
#rr=ph*rr
#ss=ss*qh

#The following code is not currently in use, but I do not want to delete it just yet.

#function topsuper(aa,rr,ss)
#cc=kron(aa[1],adjoint(transpose(rr*aa[1]*ss)));
#for i in 2:k
#cc+=kron(aa[i],adjoint(transpose(rr*aa[i]*ss)));
#end;
#return log(rad(cc));
#end;

#function bottomsuper(aa,rr,ss)
#cc=kron(rr*aa[1]*ss,adjoint(transpose(rr*aa[1]*ss)));
#for i in 2:k
#cc+=kron(rr*aa[i]*ss,adjoint(transpose(rr*aa[i]*ss)));
#end;
#return log(rad(cc));
#end;

#function super(aa,rr,ss)
#return topsuper(aa,rr,ss)-bottomsuper(aa,rr,ss)/2; 
#end;

#u=randn(ComplexF64,n,d);
#v=randn(ComplexF64,n,d);

#ubot=randn(ComplexF64,d,d);
#vbot=randn(ComplexF64,d,d);


#lambda=0;
#for iak in 1:10000
#newu=u*0;
#newv=v*0;
#for i in 1:k
#newv+=aa[i]*v*adjoint(bb[i]);
#newu+=adjoint(aa[i])*u*bb[i];
#end;
#lambda=dot(v,newv);
#u=newu/norm(newu);
#v=newv/norm(newv);
#end;

#lambdabot=0;
#for iak in 1:10000
#newu=ubot*0;
#newv=vbot*0;
#for i in 1:k
#newv+=bb[i]*vbot*adjoint(bb[i]);
#newu+=adjoint(bb[i])*ubot*bb[i];
#end;
#lambdabot=dot(vbot,newv);
#ubot=newu/norm(newu);
#vbot=newv/norm(newv);
#end;


#function rad(x) return maximum(abs.(eigvals(x))); end;
 
#function sr(aa,bb)
#cc=kron(aa[1],adjoint(transpose(bb[1])));
#for i in 2:k
#cc+=kron(aa[i],adjoint(transpose(bb[i])));
#end;
#return log(rad(cc));
#end;
 
#gradient(sr,aa,bb)[1][1]-u*bb[1]*adjoint(v)*dot(u,v)*lambda/abs(dot(u,v)*lambda)^2;
#gradient(sr,aa,bb)[2][1]-adjoint(u)*aa[1]*v*adjoint(lambda)*dot(v,u)/abs(lambda*dot(v,u))^2;

#rrgrad=rr*0;
#ssgrad=ss*0;

#for i in 1:k
#rrgrad+=adjoint(u)*aa[i]*v*adjoint(lambda)*dot(v,u)/abs(lambda*dot(v,u))^2*adjoint(aa[i]*ss);
#ssgrad+=adjoint(rr*aa[i])*adjoint(u)*aa[i]*v*adjoint(lambda)*dot(v,u)/abs(lambda*dot(v,u))^2;

#rrgrad-=ubot*bb[i]*adjoint(vbot)*dot(ubot,vbot)*lambdabot/abs(dot(ubot,vbot)*lambdabot)^2*adjoint(aa[i]*ss);
#ssgrad-=adjoint(rr*aa[i])*ubot*bb[i]*adjoint(vbot)*dot(ubot,vbot)*lambdabot/abs(dot(ubot,vbot)*lambdabot)^2;

#### The following two lines are turned off since they are equal to the above two lines.
#####rrgrad+=adjoint(ubot)*bb[i]*vbot*adjoint(lambdabot)*dot(vbot,ubot)/abs(lambdabot*dot(vbot,ubot))^2*adjoint(aa[i]*ss);
#####ssgrad+=adjoint(rr*aa[i])*adjoint(ubot)*bb[i]*vbot*adjoint(lambdabot)*dot(vbot,ubot)/abs(lambdabot*dot(vbot,ubot))^2;
#end;

#gradient(super,aa,rr,ss)[2]-rrgrad;
#gradient(super,aa,rr,ss)[3]-ssgrad;

