# Here is the code for computing the L_{2,d}-spectral radius dimensionality reduction for arbitrary sparse matrices.
# This code can be applied to analyze graphs. For example, it can be used to determine which new connections in a graph are the best.
# This code is still a work in progress. One moment.

# I am first going to set the weights to 1.

n=1000;
k=1000;
d=10;

list=[];
weights=[];
for i in 1:k
push!(list,[]);
push!(weights,rand(1:1,5));
for j in 1:5
push!(list[i],rand(1:n,2));
end;
end;

#list=[];
#weights=[];
#for i in 1:k
#push!(list,[]);
#push!(weights,[]);
#for j in 1:n
#push!(list[i],[rand(1:n),rand(1:n),rand(1:n)]);
#push!(weights[i],[randn(),randn(),randn()]);
#end;
#end;



bottomeigenvector=randn(d,d);
bottomeigenvector=bottomeigenvector/norm(bottomeigenvector);

topeigenvector=[];
for i in 1:n
push!(topeigenvector,randn(d));
end;
topeigenvector=topeigenvector/norm(topeigenvector);n

matrixnetwork=[];
for i in 1:k
push!(matrixnetwork,randn(d,d));
end;

function f(matrixnetwork)
testtopeigenvector=topeigenvector*0;
for i in 1:k
for j in 1:length(list[i])
testtopeigenvector[list[i][j][2]]+=weights[i][j]*matrixnetwork[i]*topeigenvector[list[i][j][1]];
end;
end;
return log(abs(dot(topeigenvector,testtopeigenvector)));
end;



function gradf(matrixnetwork)

testtopeigenvector=topeigenvector*0;
for i in 1:k
for j in 1:length(list[i])
testtopeigenvector[list[i][j][2]]+=weights[i][j]*matrixnetwork[i]*topeigenvector[list[i][j][1]];
end;
end;


gradientnetwork=matrixnetwork*0;
for i in 1:k
for j in 1:length(list[i])
u=topeigenvector[list[i][j]][2]];
v=weights[i][j]*topeigenvector[list[i][j][1]];
gradientnetwork[i]+=


end;
end;
end;

function g(matrixnetwork)
testbottomeigenvector=bottomeigenvector*0;
 for iar in 1:k
testbottomeigenvector+=matrixnetwork[iar]*bottomeigenvector*adjoint(matrixnetwork[iar]);
 end;

return log(norm(testbottomeigenvector)^(1/2));
end;

function h(matrixnetwork)
return f(matrixnetwork)-g(matrixnetwork);
end;

rate=1;
delta=1;
grad=[];
for i in 1:k
push!(grad,zeros(d,d));
end;

while true 
 newbottomeigenvector=bottomeigenvector*0;

 for i in 1:k
 newbottomeigenvector=newbottomeigenvector+matrixnetwork[i]*bottomeigenvector*adjoint(matrixnetwork[i]);
 end;
 bottomeigenvector=newbottomeigenvector/norm(newbottomeigenvector);
 testtopeigenvector=topeigenvector*0;

 for i in 1:k
 for j in 1:length(list[i])
 testtopeigenvector[list[i][j][2]]+=weights[i][j]*matrixnetwork[i]*topeigenvector[list[i][j][1]];
 end;
 end;

 topeigenvector=testtopeigenvector/norm(testtopeigenvector);


 grad=gradient(f,matrixnetwork)[1];

 old=f(matrixnetwork);
 matrixnetwork=matrixnetwork+rate*grad;
 new=f(matrixnetwork);

 if old>new rate=rate*0.5; end;
 rate=rate*1.01;

 squag=sum(norm.(matrixnetwork));

 matrixnetwork=matrixnetwork*(0.9+0.1/squag);
 display([new,rate,delta]);
end;
