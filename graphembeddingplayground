# Here we give examples of graph embeddings constructed using ideas similar to L_{2,d}-spectral radius dimensionality reduction.
#

#gradient(g,x,y)[1]-adjoint(u)*adjoint(v)*(u*v)*lambda/abs(u*v*lambda)^2*adjoint(y)

list=[];
type=ComplexF64;
n=200;
nx=100;
ny=100;
d=48;
e=2;
power=6;
list=[];

for i in 1:nx
for j in 1:ny
if rand(0:1)==1
push!(list,[i,j+nx]);
end;
end;
end;

ll=length(list);

#nx and ny are the dimensions of imagemat.
imagemat=zeros(256,256);
for i in 1:256
for j in 1:256
imagemat[i,j]=exp(-((i-128)^2+(j-128)^2)/10000);
end;
end;

#nx=size(imagemat)[1];
#ny=size(imagemat)[2];
n=nx+ny;
#imagemat is a 512 by 512 matrix.



#for i in 1:n
#push!(list,[]);
#end;
#for i in 1:n
#for j in 1:(i-1)
#if rand(0:1)==1
#push!(list[i],j);
#push!(list[j],i);
#end;
#end;
#end;

#ll=sum(length.(list));



matscore=function(matrix)
prod=0;
suma=zeros(d,d);
for i in 1:n
suma+=matrix[i]*adjoint(matrix[i]);
for i in 1:ll
prod+=log(norm((matrix[list[i][1]]*matrix[list[i][2]])^power));
end;
end;
return exp(prod/(ll*2*power))/norm(suma);
end;

bottommatscore=function(matrixtable)
if d==e
prod=0;
suma=zeros(d,d);
for i in 1:n
suma+=matrixtable[i]*adjoint(matrixtable[i]);
end;
return log(norm(suma))/2;
else

prod=0;
suma=zeros(d,d);
sumb=zeros(d,d);
sumc=zeros(e,e);
sumd=zeros(e,e);
for i in 1:nx
suma+=adjoint(matrixtable[i])*matrixtable[i];
end;
for i in nx+1:nx+ny
sumb+=matrixtable[i]*adjoint(matrixtable[i]);
end;
for i in 1:nx
sumc+=matrixtable[i]*adjoint(matrixtable[i]);
end;
for i in nx+1:nx+ny
sumd+=adjoint(matrixtable[i])*matrixtable[i];
end;
return (log(norm(suma)^2+norm(sumb)^2)+log(norm(sumc)^2+norm(sumd)^2))/8;
end;
end;

slowspecradmatscore=function(matrix)
prod=0;
for i in 1:ll
prod+=log(rad(matrix[list[i][1]]*matrix[list[i][2]]));
end;
return prod/(2*ll);
end;

specradmatscore=function(matrixtable)
prod=0;
for i in 1:ll
prod+=log(abs(dot(righteigenvectors[i],matrixtable[list[i][1]]*matrixtable[list[i][2]]*righteigenvectors[i])));
end;
return prod/(2*ll);
end;

compbipspecradmatscore=function(imagemat,matrixtable)
prod=0;
nx=size(imagemat)[1];
ny=size(imagemat)[2];
for i in 1:nx
for j in 1:ny
if imagemat[i,j]==0 continue; end;
prod+=log(abs(dot(righteigenvectors[i][j],matrixtable[i]*matrixtable[nx+j]*righteigenvectors[i][j])))*imagemat[i,j];
end;
end;
return prod/(2*sum(imagemat));
end;


compbiptotalspecradmatscore=function(imagemat,matrixtable)
return compbipspecradmatscore(imagemat,matrixtable)-bottommatscore(matrixtable);
end;


totalspecradmatscore=function(matrixtable)
return specradmatscore(matrixtable)-bottommatscore(matrixtable);
end;

slowtotalspecradmatscore=function(matrixtable)
return slowspecradmatscore(matrixtable)-bottommatscore(matrixtable);
end;


gradspecradmatscore=function(lefteigenvectors,righteigenvectors,matrix)
grad=[];
for i in 1:nx
push!(grad,zeros(type,e,d));
end;
for i in 1:ny
push!(grad,zeros(type,d,e));
end;
for i in 1:ll
lambda=dot(righteigenvectors[i],matrix[list[i][1]]*(matrix[list[i][2]]*righteigenvectors[i]));
cc=lefteigenvectors[i]*righteigenvectors[i];
grad[list[i][1]]+=adjoint(lefteigenvectors[i])*(adjoint(righteigenvectors[i])*adjoint(matrix[list[i][2]]))*(cc*lambda/abs(cc*lambda)^2);
grad[list[i][2]]+=(adjoint(matrix[list[i][1]])*adjoint(lefteigenvectors[i]))*adjoint(righteigenvectors[i])*(cc*lambda/abs(cc*lambda)^2);
end;
return grad/(2*ll);
end;

# Since this function is similar to gradspecradmatscore, I can probably combine these functions to save space.
compbipgradspecradmatscore=function(imagemat,lefteigenvectors,righteigenvectors,matrix)
grad=[];
for i in 1:n
push!(grad,zeros(type,d,d));
end;
for i in 1:nx
for j in 1:ny
if imagemat[i,j]==0 continue; end;
lambda=dot(righteigenvectors[i][j],matrix[i]*matrix[nx+j]*righteigenvectors[i][j]);
cc=lefteigenvectors[i][j]*righteigenvectors[i][j];
grad[i]+=adjoint(lefteigenvectors[i][j])*(adjoint(righteigenvectors[i][j])*adjoint(matrix[nx+j]))*(imagemat[i,j]*cc*lambda/abs(cc*lambda)^2);
grad[nx+j]+=(adjoint(matrix[i])*adjoint(lefteigenvectors[i][j]))*adjoint(righteigenvectors[i][j])*(imagemat[i,j]*cc*lambda/abs(cc*lambda)^2);
end;
end;
return grad/(2*sum(imagemat));
end;

compbiptotalgradspecradmatscore=function(imagemat,lefteigenvectors,righteigenvectors,matrix)
return compbipgradspecradmatscore(imagemat,lefteigenvectors,righteigenvectors,matrix)-gradient(bottommatscore,matrix)[1];
end;


totalgradspecradmatscore=function(lefteigenvectors,righteigenvectors,matrix)
return gradspecradmatscore(lefteigenvectors,righteigenvectors,matrix)-gradient(bottommatscore,matrix)[1];
end;

matrixtable=[];
for i in 1:(nx+ny)
xx=randn(type,d,d)/d^2;
push!(matrixtable,xx+xx^0);
end;

lefteigenvectors=[];
righteigenvectors=[];
for i in 1:nx
push!(lefteigenvectors,[]);
push!(righteigenvectors,[]);
for j in 1:ny
push!(lefteigenvectors[i],adjoint(randn(type,d)));
push!(righteigenvectors[i],randn(type,d));
end;
end;



rate=1;
pp=0;
qq=0;

ham=1;
mash=0;
cle=0;
while true
matrixtable=matrixtable/mean(norm.(matrixtable));

for i in 1:nx
for j in 1:ny
lefteigenvectors[i][j]=lefteigenvectors[i][j]*matrixtable[i]*matrixtable[nx+j];
righteigenvectors[i][j]=matrixtable[i]*matrixtable[nx+j]*righteigenvectors[i][j];
lefteigenvectors[i][j]=lefteigenvectors[i][j]/norm(lefteigenvectors[i][j]);
righteigenvectors[i][j]=righteigenvectors[i][j]/norm(righteigenvectors[i][j]);
end;
end;

##### GO HERE
grad=compbiptotalgradspecradmatscore(imagemat,lefteigenvectors,righteigenvectors,matrixtable)
pp=compbipspecradmatscore(imagemat,matrixtable);
qq=compbipspecradmatscore(imagemat,matrixtable+rate*grad);
if mash>pp ham+=1;
cle=0;
elseif cle>4
ham=max(1,ham-1);
end;
cle+=1;
mash=pp;
if pp>qq rate/=2; 
else
matrixtable+=rate*grad;
end;

display([rate,ham,pp]);
rate*=1.1;
qq=pp;
end;





##################
##################
##################
##################

#matrixtable=[];
#for i in 1:n
#xx=randn(type,d,d)/d^2;
#push!(matrixtable,xx+xx^0);
#end;

matrixtable=[];
for i in 1:nx
xx=randn(type,e,d);
push!(matrixtable,xx);
end;
for i in 1:ny
xx=randn(type,d,e);
push!(matrixtable,xx);
end;



lefteigenvectors=[];
righteigenvectors=[];
for i in 1:ll
push!(lefteigenvectors,adjoint(randn(type,e)));
push!(righteigenvectors,randn(type,e));
end;

rate=1;
pp=0;
qq=0;

ham=1;
mash=0;
cle=0;
while true
matrixtable=matrixtable/mean(norm.(matrixtable));

for kk in 1:ham
for i in 1:ll
lefteigenvectors[i]=lefteigenvectors[i]*matrixtable[list[i][1]]*matrixtable[list[i][2]];
righteigenvectors[i]=matrixtable[list[i][1]]*matrixtable[list[i][2]]*righteigenvectors[i];
lefteigenvectors[i]=lefteigenvectors[i]/norm(lefteigenvectors[i]);
righteigenvectors[i]=righteigenvectors[i]/norm(righteigenvectors[i]);
end;
end;

grad=totalgradspecradmatscore(lefteigenvectors,righteigenvectors,matrixtable)
pp=totalspecradmatscore(matrixtable);
qq=totalspecradmatscore(matrixtable+rate*grad);
if mash>pp ham+=1;
cle=0;
elseif cle>4
ham=max(1,ham-1);
end;
cle+=1;
mash=pp;
if pp>qq rate/=2; 
else
matrixtable+=rate*grad;
end;

display([rate,ham,pp]);
rate*=1.1;
qq=pp;
end;



arrayscore=function(leftarray,rightarray)
prod=0;
suma=zeros(d,d);
for i in 1:n
suma+=leftarray[i]*adjoint(leftarray[i])+rightarray[i]*adjoint(rightarray[i]);
for a in list[i]
prod+=log(abs(dot(leftarray[i],rightarray[a])));
end;
end;
return exp(prod/ll)/norm(suma);
end;
qq=0;
rate=.3;


# The algorithm for finding the gradient of the spectral radius requires one to have a good approximation for a single dominant eigenvalue.
# For real matrices, the dominant eigenvalues may be of the form a+bi,a-bi with a,b real. We either need to use complex matrices during training or change the code so
# that it can better compute the gradient of the spectral radius of real matrices. Another problem is that the dominant eigenvectors of a real matrix may be complex
# Since we are using complex matrices for the dominant eigenvectors, we might as well use complex matrices during the training.

while true


for i in 1:ll
lefteigenvectors[i]=lefteigenvectors[i]*matrixtable[list[i][1]]*matrixtable[list[i][2]];
righteigenvectors[i]=matrixtable[list[i][1]]*matrixtable[list[i][2]]*righteigenvectors[i];
lefteigenvectors[i]=lefteigenvectors[i]/norm(lefteigenvectors[i]);
righteigenvectors[i]=righteigenvectors[i]/norm(righteigenvectors[i]);
end;


end;


rate=1;
pp=1;
qq=1;
amm=0;
while true
#  for i in 1:n
#  for j in 1:dr
#  for k in 1:dr
#  matrixtable[i][2*j,2*k-1]=-adjoint(matrixtable[i][2*j-1,2*k]);
#  matrixtable[i][2*j,2*k]=adjoint(matrixtable[i][2*j-1,2*k-1]);
#  end;
#  end;
#  end;

grad=gradient(matscore,matrixtable)[1];
matrixtable=matrixtable+rate*grad;
matrixtable/=mean(norm.(matrixtable));
pp=matscore(matrixtable);
amm=amm*0.99+0.01*(pp-qq);
display([rate,pp,(pp-qq)/amm]);
if qq>pp rate/=2;
end;
rate*=1.01;
qq=pp;
end;


rate=1;
pp=1;
qq=1;
amm=0;
while true
grad=gradient(score,leftarray,rightarray);
leftarray+=rate*grad[1];
rightarray+=rate*grad[2];
amm=norm(leftarray);
leftarray/=amm;
rightarray/=amm;
pp=score(leftarray,rightarray);
amm=amm*0.99+0.01*(pp-qq);
display([rate,pp,(pp-qq)/amm]);
#if qq>pp rate/=2;
#end;
#rate*=1.01;
qq=pp;
end;


