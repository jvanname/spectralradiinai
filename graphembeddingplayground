# Here we give examples of graph embeddings constructed using ideas similar to L_{2,d}-spectral radius dimensionality reduction.

# I am going to have to write the code for when the matrix is factored as a product of low rank matrices.

#gradient(g,x,y)[1]-adjoint(u)*adjoint(v)*(u*v)*lambda/abs(u*v*lambda)^2*adjoint(y)

list=[];
type=ComplexF64;
n=80;
d=2;
power=6;

list=[];

for i in 1:n
for j in 1:(i-1)
if ((i-40)^2+(j-40)^2)&200>100
push!(list,[i,j]); 
end;
end;
end;
ll=length(list);

#for i in 1:n
#push!(list,[]);
#end;
#for i in 1:n
#for j in 1:(i-1)
#if rand(0:1)==1
#push!(list[i],j);
#push!(list[j],i);
#end;
#end;
#end;

#ll=sum(length.(list));



matscore=function(matrix)
prod=0;
suma=zeros(d,d);
for i in 1:n
suma+=matrix[i]*adjoint(matrix[i]);
for a in list[i]
prod+=log(norm((matrix[a]*matrix[i])^power));
end;
end;
return exp(prod/(ll*2*power))/norm(suma);
end;

bottommatscore=function(matrix)
prod=0;
suma=zeros(d,d);
for i in 1:n
suma+=matrix[i]*adjoint(matrix[i]);
end;
return log(norm(suma))/2;
end;

slowspecradmatscore=function(matrix)
prod=0;
for i in 1:ll
prod+=log(rad(matrix[list[i][1]]*matrix[list[i][2]]));
end;
return prod/(2*ll);
end;

specradmatscore=function(matrixtable)
prod=0;
for i in 1:ll
prod+=log(abs(dot(righteigenvectors[i],matrixtable[list[i][1]]*matrixtable[list[i][2]]*righteigenvectors[i])));
end;
return prod/(2*ll);
end;

totalspecradmatscore=function(matrixtable)
return specradmatscore(matrixtable)-bottommatscore(matrixtable);
end;

slowtotalspecradmatscore=function(matrixtable)
return slowspecradmatscore(matrixtable)-bottommatscore(matrixtable);
end;


gradspecradmatscore=function(lefteigenvectors,righteigenvectors,matrix)
grad=[];
for i in 1:n
push!(grad,zeros(type,d,d));
end;
for i in 1:ll
lambda=dot(righteigenvectors[i],matrix[list[i][1]]*matrix[list[i][2]]*righteigenvectors[i]);
cc=lefteigenvectors[i]*righteigenvectors[i];
grad[list[i][1]]+=adjoint(lefteigenvectors[i])*(adjoint(righteigenvectors[i])*adjoint(matrix[list[i][2]]))*(cc*lambda/abs(cc*lambda)^2);
grad[list[i][2]]+=(adjoint(matrix[list[i][1]])*adjoint(lefteigenvectors[i]))*adjoint(righteigenvectors[i])*(cc*lambda/abs(cc*lambda)^2);
end;
return grad/(2*ll);
end;



totalgradspecradmatscore=function(lefteigenvectors,righteigenvectors,matrix)
return gradspecradmatscore(lefteigenvectors,righteigenvectors,matrix)-gradient(bottommatscore,matrix)[1];
end;

matrixtable=[];
for i in 1:n
xx=randn(type,d,d);
push!(matrixtable,xx);
end;

lefteigenvectors=[];
righteigenvectors=[];
for i in 1:ll
push!(lefteigenvectors,adjoint(randn(type,d)));
push!(righteigenvectors,randn(type,d));
end;


#for kk in 1:1000
#for i in 1:ll
#lefteigenvectors[i]=lefteigenvectors[i]*matrixtable[list[i][1]]*matrixtable[list[i][2]];
#righteigenvectors[i]=matrixtable[list[i][1]]*matrixtable[list[i][2]]*righteigenvectors[i];
#lefteigenvectors[i]=lefteigenvectors[i]/norm(lefteigenvectors[i]);
#righteigenvectors[i]=righteigenvectors[i]/norm(righteigenvectors[i]);
#end;
#end;

rate=0.01;
pp=0;
qq=0;
while true
matrixtable=matrixtable/mean(norm.(matrixtable));

for i in 1:ll
lefteigenvectors[i]=lefteigenvectors[i]*matrixtable[list[i][1]]*matrixtable[list[i][2]];
righteigenvectors[i]=matrixtable[list[i][1]]*matrixtable[list[i][2]]*righteigenvectors[i];
lefteigenvectors[i]=lefteigenvectors[i]/norm(lefteigenvectors[i]);
righteigenvectors[i]=righteigenvectors[i]/norm(righteigenvectors[i]);
end;

grad=totalgradspecradmatscore(lefteigenvectors,righteigenvectors,matrixtable)
pp=totalspecradmatscore(matrixtable);
qq=totalspecradmatscore(matrixtable+rate*grad);
if pp>qq rate/=2; 
else
matrixtable+=rate*grad;
end;

display([rate,pp]);
rate*=1.01;
qq=pp;
end;



arrayscore=function(leftarray,rightarray)
prod=0;
suma=zeros(d,d);
for i in 1:n
suma+=leftarray[i]*adjoint(leftarray[i])+rightarray[i]*adjoint(rightarray[i]);
for a in list[i]
prod+=log(abs(dot(leftarray[i],rightarray[a])));
end;
end;
return exp(prod/ll)/norm(suma);
end;
qq=0;
rate=.3;


# The algorithm for finding the gradient of the spectral radius requires one to have a good approximation for a single dominant eigenvalue.
# For real matrices, the dominant eigenvalues may be of the form a+bi,a-bi with a,b real. We either need to use complex matrices during training or change the code so
# that it can better compute the gradient of the spectral radius of real matrices. Another problem is that the dominant eigenvectors of a real matrix may be complex
# Since we are using complex matrices for the dominant eigenvectors, we might as well use complex matrices during the training.

while true


for i in 1:ll
lefteigenvectors[i]=lefteigenvectors[i]*matrixtable[list[i][1]]*matrixtable[list[i][2]];
righteigenvectors[i]=matrixtable[list[i][1]]*matrixtable[list[i][2]]*righteigenvectors[i];
lefteigenvectors[i]=lefteigenvectors[i]/norm(lefteigenvectors[i]);
righteigenvectors[i]=righteigenvectors[i]/norm(righteigenvectors[i]);
end;


end;


rate=1;
pp=1;
qq=1;
amm=0;
while true
#  for i in 1:n
#  for j in 1:dr
#  for k in 1:dr
#  matrixtable[i][2*j,2*k-1]=-adjoint(matrixtable[i][2*j-1,2*k]);
#  matrixtable[i][2*j,2*k]=adjoint(matrixtable[i][2*j-1,2*k-1]);
#  end;
#  end;
#  end;

grad=gradient(matscore,matrixtable)[1];
matrixtable=matrixtable+rate*grad;
matrixtable/=mean(norm.(matrixtable));
pp=matscore(matrixtable);
amm=amm*0.99+0.01*(pp-qq);
display([rate,pp,(pp-qq)/amm]);
if qq>pp rate/=2;
end;
rate*=1.01;
qq=pp;
end;


rate=1;
pp=1;
qq=1;
amm=0;
while true
grad=gradient(score,leftarray,rightarray);
leftarray+=rate*grad[1];
rightarray+=rate*grad[2];
amm=norm(leftarray);
leftarray/=amm;
rightarray/=amm;
pp=score(leftarray,rightarray);
amm=amm*0.99+0.01*(pp-qq);
display([rate,pp,(pp-qq)/amm]);
#if qq>pp rate/=2;
#end;
#rate*=1.01;
qq=pp;
end;


